"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[138],{1380:(o,e,t)=>{t.r(e),t.d(e,{assets:()=>r,contentTitle:()=>c,default:()=>d,frontMatter:()=>i,metadata:()=>n,toc:()=>u});const n=JSON.parse('{"id":"module-4-vla","title":"Module 4: Vision-Language-Action (VLA)","description":"Convergence of LLMs and Robotics; Voice-to-Action Using LLMs to translate natural language into ROS 2 actions.","source":"@site/docs/05-module-4-vla.mdx","sourceDirName":".","slug":"/module-4-vla","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-4-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/05-module-4-vla.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Module 4: Vision-Language-Action (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-3-isaac"},"next":{"title":"Capstone: The Autonomous Humanoid","permalink":"/Physical-AI-Humanoid-Robotics/docs/capstone"}}');var a=t(4848),s=t(8453);const i={title:"Module 4: Vision-Language-Action (VLA)"},c=void 0,r={},u=[];function l(o){const e={p:"p",...(0,s.R)(),...o.components};return(0,a.jsx)(e.p,{children:"Convergence of LLMs and Robotics; Voice-to-Action: Using OpenAI Whisper for voice commands; Cognitive Planning: Using LLMs to translate natural language into ROS 2 actions."})}function d(o={}){const{wrapper:e}={...(0,s.R)(),...o.components};return e?(0,a.jsx)(e,{...o,children:(0,a.jsx)(l,{...o})}):l(o)}},8453:(o,e,t)=>{t.d(e,{R:()=>i,x:()=>c});var n=t(6540);const a={},s=n.createContext(a);function i(o){const e=n.useContext(s);return n.useMemo(function(){return"function"==typeof o?o(e):{...e,...o}},[e,o])}function c(o){let e;return e=o.disableParentContext?"function"==typeof o.components?o.components(a):o.components||a:i(o.components),n.createElement(s.Provider,{value:e},o.children)}}}]);