"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[138],{1380:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla","title":"Module 4: Vision-Language-Action (VLA)","description":"This is where the magic happens! In this module, we will explore Vision-Language-Action (VLA) models, which represent the convergence of Large Language Models (LLMs) like GPT and the field of Robotics.","source":"@site/docs/05-module-4-vla.mdx","sourceDirName":".","slug":"/module-4-vla","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-4-vla","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Module 4: Vision-Language-Action (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-3-isaac"},"next":{"title":"Capstone: The Autonomous Humanoid","permalink":"/Physical-AI-Humanoid-Robotics/docs/capstone"}}');var i=o(4848),s=o(8453);const a={sidebar_position:5,title:"Module 4: Vision-Language-Action (VLA)"},l="Module 4: Vision-Language-Action (VLA)",r={},c=[{value:"Focus: The Convergence of LLMs and Robotics",id:"focus-the-convergence-of-llms-and-robotics",level:3},{value:"Core Concepts",id:"core-concepts",level:3},{value:"Example: A Simple LLM Prompt for Planning",id:"example-a-simple-llm-prompt-for-planning",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,i.jsxs)(n.p,{children:["This is where the magic happens! In this module, we will explore ",(0,i.jsx)(n.strong,{children:"Vision-Language-Action (VLA) models"}),", which represent the convergence of Large Language Models (LLMs) like GPT and the field of Robotics."]}),"\n",(0,i.jsx)(n.h3,{id:"focus-the-convergence-of-llms-and-robotics",children:"Focus: The Convergence of LLMs and Robotics"}),"\n",(0,i.jsxs)(n.p,{children:["The goal of VLA is to enable robots to understand and execute commands given in natural human language. Instead of writing complex code for every task, we can simply ",(0,i.jsx)(n.em,{children:"tell"})," the robot what to do."]}),"\n",(0,i.jsx)(n.h3,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Voice-to-Action: Using OpenAI Whisper"}),"\r\nWe will start by capturing voice commands using a microphone. The audio will be transcribed into text using ",(0,i.jsx)(n.strong,{children:"OpenAI Whisper"}),', a state-of-the-art speech-to-text model. This is the "Language" part of VLA.']}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cognitive Planning: Using LLMs"}),'\r\nOnce we have the text command (e.g., "Please bring me the red apple from the table"), we will feed it to a Large Language Model. The LLM\'s job is to act as a "Cognitive Planner", breaking down the high-level command into a sequence of simple, actionable steps that a robot can understand.']}),"\n",(0,i.jsx)(n.p,{children:'For example, "Bring me the red apple" might be broken down into:'}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"Step 1: Navigate to the table."})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"Step 2: Use the camera to locate the red apple (Vision)."})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"Step 3: Calculate the grasping position."})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"Step 4: Move the arm and pick up the apple."})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"Step 5: Navigate back to the user."})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Translating Plans into ROS 2 Actions"}),"\r\nFinally, we will write a Python script that takes the step-by-step plan from the LLM and translates each step into a specific ",(0,i.jsx)(n.strong,{children:"ROS 2 Action"})," or ",(0,i.jsx)(n.strong,{children:"Service call"}),'. This is the "Action" part of VLA, where the digital plan becomes a physical movement.']}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-a-simple-llm-prompt-for-planning",children:"Example: A Simple LLM Prompt for Planning"}),"\n",(0,i.jsx)(n.p,{children:"We can use an LLM API to generate a plan. Here's what a prompt might look like:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",metastring:'title="Example Prompt to an LLM"',children:'{\r\n  "role": "system",\r\n  "content": "You are a helpful robot assistant. Convert the user\'s command into a numbered list of simple robot actions from this list: [navigate(location), find(object), grasp(object), place(object, location)]."\r\n}\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",metastring:'title="User Command"',children:'{\r\n  "role": "user",\r\n  "content": "Clean the room by putting the toy car in the box."\r\n}\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",metastring:'title="Expected LLM Response"',children:'{\r\n  "role": "assistant",\r\n  "content": "1. find(toy car)\\n2. grasp(toy car)\\n3. navigate(box)\\n4. place(toy car, box)"\r\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:"This module connects the world of conversational AI with physical robotics, allowing for truly natural and intuitive human-robot interaction."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>l});var t=o(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);